# **Augmenting the Gallery - Notes**
## Class Notes
### 2021/02/02

Lowering the bars of entering subjects and industries

Knowledge in relation to the physical world

Paper -> one of the most two-dimension

## Reading Responses
### week 02

#### On Licklider’s *Man-Computer Symbiosis*

In this essay Licklider presents the current (well, his “current” by the time this was written) absence of and a ideal vision of a much more smooth, intuitive, “organic” interaction between human and computer. He calls it “symbiosis” much like his example of fig tree and one kind of wasp. Naturally we may come up with the doubt that computer, does not really need human. Certainly, computers require power  in order to operate properly, but the very reason computers are made and then plugged into the power grid is to serve us in one way or another. Computers do not rely on human, at least not in the same way as we rely on computers. But, if we steer away from this “consciousness” philosophical dilemma, Licklider does offer a good discourse on how human and machine can cooperate more efficiently, that is to combine the respective strengths of the two and supplement each other, for a collective goal. (Again, not that a machine needs a goal.) 

This ideal situation differs from “mechanically extended man” or “semi-automatic systems” where there are few interactions. This situation requires constant interactions between man and computer that will form loops of information exchange which not only tackle mechanical and repetitive work but also will foster the process of decision-making or interllectual thinking. I am particularly font of Licklider’s idea that such interaction resembles working with “a colleague whose competence supplements your own”. In order to achieve such level of cooperation, man and computer should at least communicate seamlessly. 

Again, we may gain better understanding of it by making another analogy. Think of man and computer as two people who speak different languages, and they would work more closely and efficiently through a third language that both of them speak, or body language that to some extent convey meanings universally. And that is the signifance of inventions like the graphical user interface, touch screen, Scratch language, or even any basic high-level programming language to some extent. They, as the “third language”, serve as a midpoint between the world of human and the world of machine where the two could meet with fewer obstacles. 

Over the years, this midpoint has shifted more and more towards the world of us, of course, for all these works are set to be done in our favor. If we look at this matter of man-machine communication in a “human-centralism”, such shift enables more people that aren’t so familiar with technology to enhance their living experience more seamlessly with the help of computers and other electronic devices that are being invented daily nowadays without much learning cost.

#### On Sutherland’s *The Ultimate Display*

This is without doubt a piece of writing with limits of its time. Most of the technical difficulties found in the making of computer displays and inventions of new means of interacting with computers through display, that are mentioned in Sutherland’s essay have been conquered over the years. Many concepts imagined in this essay have long infiltrated the mundane life, some other have been proven obsolete or fallen into disuse. 

Touchscreen, as a huge part of Sutherland’s research, even surpasses “the typewriter keyboard” which Sutherland himself considered what would be the main tool to communicate with computers, as the primary input device for almost all electronics. Touchscreen becomes the absolute dominating tool to interact with a smartphone; it is implemented on new generations of laptops; it is implemented on treadmills; it even makes its appearance on the center console of the car to replace physical control buttons and knobs. 

One advantage of touch screen is that it introduces a more natural way of interacting with the machine. The desire of touching on something after seeing it might one of our instincts that we are born with. And that leads to the idea of overlapped mappings (my made-up word). We are able to immerse more into the experience when more actions from us are performed under consciousness. In the instance of touch screen, that is to say, I know and I will be touching and manipulating the thing that I am currently looking at. 

Communicating with a computer using a keyboard, especially a few decades ago, requires the person to trigger a series of keys on the keyboard, that is in front of the person, on the desk presumably,  in order to input a series of command lines, that is not in the form of natural language most likely, which will be received by the computer and translated into visual signals on a seperate display further away from the person. A huge amount of ciphering and deciphering, and language learning are involved in such process which will itimidate many people with less knowledge of computer and pull down the fluency of the workflow drastically in many situations. 

Touch screen allows the input and output to happen on the very same spot and nearly the same time. As the output display changes its content, for example, after a command was given through a touch, the potential input commands projected onto the display also change accordingly, allowing for a much more flexible and efficient control on the device. And unlike using a keyboard, or a mouse, or a joystick, this process doesn’t need extra matching of the mapping of the input and the one of the output on the user’s end; a event will be triggered exactly at where I touched the screen and a visual (or some other forms of) response will take place potentially in that very spot. 

#### On Krevelen’s *A Survey of Augmented Reality*

Krevelen believes that Augmented Reality can “enhance our perception” and help people to see, hear, feel our enviornments more than other can. However, it seems that AR currently hasn’t quite realised such attractive promise. Admittedly, AR does find its place in certain fields like industrial assembly lines and sports broadcasting as mentioned in the essay, but it is yet far from being a handy tool that will meet the demands of ordinary people. 

Virtual Reality has made an increasing appearance in the gaming and cinema industries one the years. People have figured out where to position the VR technology in our cultural world. In other words, VR has found its meaning in certain fields and a healthy circulation of production and demand is already established. In the case of AR, however, people are still struggling in many situations to realize the potential of it. “What is the question” for AR? What’s left undone in our everyday life? 

To advance in this question, allow me to make the challenge more specific. As there should be an inevitable integration of a real environment and virtual objects (or say information in general), the accessibility of AR is extremely crucial in making it actually handy for the application scenario most likely involves moving around a physical space. I try to use the word “accessibility” instead of “portability” for the reason follows: A smartphone is made extremely portable but we don’t want to pick up it up every time we wants something from it and pause what we are doing at the time. Many commands are be done with our hands off, or even when holding the phone, it doesn’t interrupt with the work at hand. In other words, using AR should ideally introduce as little cost (in every aspect) as possible. It shouldn’t cost a fortune to use; It shouldn’t introduce many extra physical devices; It shouldn’t require a ton of action for us to use; It shouldn’t cost us much extra time. 

Integrating AR into smartphones has solved most of the mentioned above, but a critical flaw can be found in many (if not all) of the AR applications: they could be introducing an extra step of action and disrupting, instead of enhancing, our real time experience. To give a specific example, imaging an AR application for museums. As one holds up his/her phone and points the back camera to the artwork in front, all sorts of relevant information, whether in text or in visual, pop up on the phone screen. Maybe, the information is even carefully mapped onto the artwork’s surface, or its wall label. The experience sounds intriguing until one realizes that it largely affects the viewing experience, the experience of standing directly in front of an artwork that exists in the exact place and time. And this as well is why many museum-going enthusiasts belittle those who do nothing but to take photos of every artworks they walk by in a museum. When a physical artwork is transformed into a two-dimensional image on the screen, much of its charm could be lost and never rescued. We then don’t “see more than others see”, rather our vision is potentially shrunk down into a small piece screen. The act of holding up the phone in front of the artwork, disrupt our normal visual experience in the museum. 

And part of the reason why audio guide is introduced into museums is that wearing headphones doesn’t conflict with our visual experience and auditory information can flow into our ears while we are seeing the artworks directly with our eyes. That’s why I think Siri is by far the most successful AR application, if we define AR generously. Or, audio guide IS a kind of augmented reality. It seems that when people talk about AR, too much attention is put into the visual while the audio and other senses are easily overlooked. 

Nevertheless, the problems found in the attempts of augmenting the visual reality still remain unsolved. How can we archive the level of success in the audio version of AR for instance? An effective antidote is hard to offer at this point. But back to the museum scenario, how can we fit our AR application into moments and bits when people actually will take out their phones so that it can run more transparently?

### week 03

